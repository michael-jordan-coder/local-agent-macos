# PROJECT_MEMORY.md — LocalAssistant

## 1. Project Purpose

- Native macOS desktop chat client for locally-running Ollama LLM server
- Polished SwiftUI interface for interacting with local language models
- No cloud dependencies — all inference on-device via Ollama at `127.0.0.1:11434`
- Target user: developer or power user running Ollama locally on macOS

## 2. Architecture Overview

- **Pattern:** MVVM with `@Observable` macro (modern SwiftUI, not ObservableObject)
- **Layers:** Views → ViewModels (@Observable, @MainActor) → Services (stateless structs) → OllamaClient → HTTP API
- **Runtime:** Single-window macOS app, NavigationSplitView (sidebar + detail)
- **Streaming:** Ollama responses stream via `URLSession.bytes`, tokens batched (~180 chars or ~24ms) before flushing to UI for reduced re-renders
- **Summarization:** Auto-triggers when conversation exceeds 40 messages; trims to last 16 + summary; input truncated to 16k chars
- **Persistence:** JSON files per conversation in Application Support; summary as plain text; settings in UserDefaults

### Data Flow

```
User input → ChatViewModel.send() → ImageProcessor (downscale+cache) → PromptBuilder.build() → OllamaClient.streamGenerate()
    → tokens batched and flushed to UI → conversation saved → auto-summarize if threshold exceeded
```

### Prompt Construction (PromptBuilder)

Assembled as: `[SYSTEM]` + `[SUMMARY]` + `[CONVERSATION]` (recent 16 messages) + `[REFERENCED]` (optional mention) + new user message

System instructions stored as a static `let` — computed once, reused every call.

## 3. Technology Stack

- **Language:** Swift
- **UI:** SwiftUI (macOS 26.2)
- **Build:** Xcode / .xcodeproj
- **Dependencies:** MarkdownUI (assistant message rendering), CryptoKit (image hashing), ServiceManagement (background launch). No other external deps.
- **Bundle ID:** `daniels.LocalAssistant`

## 4. Directory Map

```
App/            → LocalAssistantApp.swift — entry point, singleton service creation, scene setup
Models/         → ChatMessage, Conversation — Codable data structs
ViewModels/     → ChatViewModel, AppStatusViewModel, SummaryViewModel — @Observable @MainActor state
Views/          → ContentView, ChatView, ComposerView, MessageRowView, EmptyStateView,
                  StatusBarView, SystemPromptPanelView, SettingsView
Services/       → OllamaClient, PromptBuilder, ChatPersistence, SummarizationService, ImageProcessor
```

## 5. Core Concepts and Mental Models

- **Conversation:** Container with UUID, title, messages array, creation date, optional per-conversation `systemPrompt`. One JSON file per conversation.
- **ChatMessage:** Has role (`"user"`, `"assistant"`, `"system"`), content, optional image attachments as `[Data]`, timestamp, optional `mentionPreview` (shown on user messages that reference past answers).
- **Per-Conversation System Prompt:** Stored as `Conversation.systemPrompt: String?`. Persisted in conversation JSON. UI reads/writes via `ChatViewModel.currentSystemPrompt`, `applySystemPrompt(_:)`, `resetSystemPrompt()`. Old JSON files without this field decode with `nil` (backward compatible).
- **Mention Feature:** User can reference past assistant messages. Activated via quote button → pick mode (messages become clickable in chat). Selected mention stored as `ChatViewModel.mentionedMessage`, displayed as chip in composer. On send, first 500 chars injected as `[REFERENCED]` section in prompt, and an 80-char preview saved in the user message's `mentionPreview` field.
- **Summary:** Single global summary text file. Generated by asking the LLM to summarize old messages into ≤12 bullet points.
- **Summarization thresholds:** Trigger at 40 messages, keep last 16 after trim.
- **OllamaClient:** Stateless struct with shared `URLSession` (connection reuse, 120s timeouts, max 2 connections per host). All HTTP calls are async/throws. Streaming uses JSONL with `Task.isCancelled` checks for responsive cancellation. Keep-alive header on streaming requests.
- **PromptBuilder:** Static enum utility. Formats the full prompt string from system + summary + conversation + referenced + user message parts. Trims whitespace on all sections.
- **ImageProcessor:** Downscales images to max 1024px longest edge (JPEG 82%), SHA256-keyed in-memory base64 cache to avoid re-encoding identical images. Images pre-processed before network call.
- **Copy button:** Always-visible button below every assistant message. Shows "Copy"/"Copied" with checkmark animation on click.

## 6. Critical Files and Entry Points

| File | Why It Matters |
|------|---------------|
| `App/LocalAssistantApp.swift` | Creates all services and ViewModels, wires dependencies, defines scenes |
| `ViewModels/ChatViewModel.swift` | Central orchestrator — sending, streaming (batched), persistence, summarization, mention state, system prompt |
| `Services/OllamaClient.swift` | All HTTP communication with Ollama (generate, stream, models, reachability, launch). Shared URLSession. |
| `Services/PromptBuilder.swift` | Prompt assembly logic — static system prompt, optional referenced context |
| `Services/ChatPersistence.swift` | JSON read/write for conversations |
| `Services/ImageProcessor.swift` | Image downscaling, base64 encoding with SHA256 cache |
| `Services/SummarizationService.swift` | Summary generation with input truncation (16k chars) |
| `Views/ContentView.swift` | Root layout — sidebar + detail split, toolbar, inspector, mention pick-mode wiring |
| `Views/MessageRowView.swift` | Message rendering — markdown theme, code blocks, image display, copy button, mention pick interaction, spinner |
| `Views/ComposerView.swift` | Input area — text field, image attachment, mention button + chip, send/stop |
| `Views/ChatView.swift` | Scrollable message list — passes pick-mode state and callback to MessageRowView |
| `Models/ChatMessage.swift` | Message model — role, content, images, timestamp, mentionPreview |
| `Models/Conversation.swift` | Conversation model — id, title, messages, createdAt, systemPrompt |

## 7. Constraints and Tradeoffs

- **No app sandbox** — required for process execution (launching Ollama) and local network access
- **Hardcoded Ollama URL** — `http://127.0.0.1:11434`, no remote server support
- **Default model** — `"llama3"` in UserDefaults, changeable in Settings via fetched model list
- **Silent I/O failures** — persistence uses `try?` throughout; errors are swallowed
- **Single global summary** — shared across all conversations, not per-conversation
- **120-second HTTP timeout** — configured on shared URLSession, applies to both streaming and non-streaming
- **No Combine** — pure async/await throughout
- **All ViewModels are @MainActor** — thread safety via actor isolation
- **Image encoding** — binary Data in model, downscaled + base64-cached by ImageProcessor before sending to Ollama API
- **Markdown rendering** — MarkdownUI with custom `.assistantMessage` theme (tuned list spacing, blockquotes, tables, paragraph rhythm); code blocks manually parsed from fences for dedicated CodeBlockView with copy button
- **Ollama launch paths tried:** `/usr/local/bin/ollama`, `/opt/homebrew/bin/ollama`, `/usr/bin/env ollama`
- **Startup:** 12-second timeout, polls every 400ms
- **Streaming batching:** Flushes to UI every ~180 chars or ~24ms to reduce SwiftUI re-renders
- **Mention context:** Truncated to 500 chars in prompt, 80 chars for user message preview

### Persistence Locations

- Conversations: `~/Library/Application Support/LocalAssistant/conversations/{UUID}.json`
- Summary: `~/Library/Application Support/LocalAssistant/summary.txt`
- Settings: UserDefaults (`appTheme`, `selectedModel`)
