# PROJECT_MEMORY.md — LocalAssistant

## 1. Project Purpose

- Native macOS desktop chat client for locally-running Ollama LLM server
- Polished SwiftUI interface for interacting with local language models
- No cloud dependencies — all inference on-device via Ollama at `127.0.0.1:11434`
- Target user: developer or power user running Ollama locally on macOS

## 2. Architecture Overview

- **Pattern:** MVVM with `@Observable` macro (modern SwiftUI, not ObservableObject)
- **Layers:** Views → ViewModels (@Observable, @MainActor) → Services (stateless structs) → OllamaClient → HTTP API
- **Runtime:** Single-window macOS app, NavigationSplitView (sidebar + detail)
- **Streaming:** Ollama responses stream via `URLSession.bytes`, tokens delivered to UI in real-time via @MainActor callbacks
- **Summarization:** Auto-triggers when conversation exceeds 40 messages; trims to last 16 + summary
- **Persistence:** JSON files per conversation in Application Support; summary as plain text; settings in UserDefaults

### Data Flow

```
User input → ChatViewModel.send() → PromptBuilder.build() → OllamaClient.streamGenerate()
    → tokens streamed to UI → conversation saved → auto-summarize if threshold exceeded
```

### Prompt Construction (PromptBuilder)

Assembled as: `[SYSTEM]` + `[SUMMARY]` + `[CONVERSATION]` (recent 16 messages) + new user message

## 3. Technology Stack

- **Language:** Swift
- **UI:** SwiftUI (macOS 26.2)
- **Build:** Xcode / .xcodeproj
- **Dependencies:** MarkdownUI (assistant message rendering), ServiceManagement (background launch). No other external deps.
- **Bundle ID:** `daniels.LocalAssistant`

## 4. Directory Map

```
App/            → LocalAssistantApp.swift — entry point, singleton service creation, scene setup
Models/         → ChatMessage, Conversation — Codable data structs
ViewModels/     → ChatViewModel, AppStatusViewModel, SummaryViewModel — @Observable @MainActor state
Views/          → ContentView, ChatView, ComposerView, MessageRowView, EmptyStateView,
                  StatusBarView, SystemPromptPanelView, SettingsView
Services/       → OllamaClient, PromptBuilder, ChatPersistence, SummarizationService
```

## 5. Core Concepts and Mental Models

- **Conversation:** Container with UUID, title, messages array, creation date. One JSON file per conversation.
- **ChatMessage:** Has role (`"user"`, `"assistant"`, `"system"`), content, optional image attachments as `[Data]`, timestamp.
- **Session System Prompt:** Per-session override of system instructions. Not persisted — resets on app restart.
- **Summary:** Single global summary text file. Generated by asking the LLM to summarize old messages into ≤12 bullet points.
- **Summarization thresholds:** Trigger at 40 messages, keep last 16 after trim.
- **OllamaClient:** Stateless struct. All HTTP calls are async/throws. Streaming uses JSONL (`{ "response": "token", "done": bool }`).
- **PromptBuilder:** Static enum utility. Formats the full prompt string from system + summary + conversation parts.

## 6. Critical Files and Entry Points

| File | Why It Matters |
|------|---------------|
| `App/LocalAssistantApp.swift` | Creates all services and ViewModels, wires dependencies, defines scenes |
| `ViewModels/ChatViewModel.swift` | Central orchestrator — sending, streaming, persistence, summarization |
| `Services/OllamaClient.swift` | All HTTP communication with Ollama (generate, stream, models, reachability, launch) |
| `Services/PromptBuilder.swift` | Prompt assembly logic — hardcoded system prompt text lives here |
| `Services/ChatPersistence.swift` | JSON read/write for conversations |
| `Views/ContentView.swift` | Root layout — sidebar + detail split, toolbar, inspector |
| `Views/MessageRowView.swift` | Message rendering — markdown theme, code blocks, image display, spinner |
| `Views/ComposerView.swift` | Input area — text field, image attachment, send/stop |

## 7. Constraints and Tradeoffs

- **No app sandbox** — required for process execution (launching Ollama) and local network access
- **Hardcoded Ollama URL** — `http://127.0.0.1:11434`, no remote server support
- **Default model** — `"llama3"` in UserDefaults, changeable in Settings via fetched model list
- **Silent I/O failures** — persistence uses `try?` throughout; errors are swallowed
- **Single global summary** — shared across all conversations, not per-conversation
- **120-second HTTP timeout** — applies to both streaming and non-streaming calls
- **No Combine** — pure async/await throughout
- **All ViewModels are @MainActor** — thread safety via actor isolation
- **Image encoding** — binary Data in model, base64-encoded when sent to Ollama API
- **Markdown rendering** — MarkdownUI with custom `.assistantMessage` theme; code blocks manually parsed from fences for dedicated CodeBlockView with copy button
- **Ollama launch paths tried:** `/usr/local/bin/ollama`, `/opt/homebrew/bin/ollama`, `/usr/bin/env ollama`
- **Startup:** 12-second timeout, polls every 400ms

### Persistence Locations

- Conversations: `~/Library/Application Support/LocalAssistant/conversations/{UUID}.json`
- Summary: `~/Library/Application Support/LocalAssistant/summary.txt`
- Settings: UserDefaults (`appTheme`, `selectedModel`)
